{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run options\n",
    "Set `recalc` to True to force recalculating all results, otherwise, load the saved results from file.\n",
    "\n",
    "Choose `easy` or `hard` to specify which dataset to test over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalc = True\n",
    "difficulty = \"easy\"  # or \"hard\"\n",
    "\n",
    "SIM_EVALS = 10     # PAPER VALUE: 100\n",
    "tuning_iters = 10  # PAPER VALUE: 100\n",
    "TUNENET_EPOCHS = 10 # PAPER VALUE: 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tune.definitions import ROOT_DIR, OUTPUT_DIR\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import importlib\n",
    "from tune.utils import save_files, load_files, get_torch_device\n",
    "from tune.train_tunenet_gt import INPUT_DIM, OUT_DIM, BATCH_SIZE\n",
    "import tune.train_tunenet_gt\n",
    "from tune.model_tunenet import TuneNet\n",
    "import torch\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext jupyternotify\n",
    "\n",
    "output_path = os.path.join(ROOT_DIR, OUTPUT_DIR)\n",
    "print(ROOT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kind of sim are we going to run?\n",
    "from tune.ball_sim import BallSim\n",
    "from tune.dataset_tunenet import DatasetTuneNet\n",
    "importlib.reload(tune.ball_sim)\n",
    "importlib.reload(tune.dataset_tunenet)\n",
    "\n",
    "SimType = BallSim\n",
    "\n",
    "if difficulty == \"easy\":\n",
    "    prefix, test_loader, graph_title =  \\\n",
    "        \"ball_gt\", DatasetTuneNet.get_data_loader(\"tune\", \"ground_truth\", \"val\", BATCH_SIZE), \"GT Easy\"\n",
    "elif difficulty == \"hard\":\n",
    "    prefix, test_loader, graph_title = \\\n",
    "       \"ball_gt_hard\", DatasetTuneNet.get_data_loader(\"tune_hard\", \"ground_truth\", \"val\", BATCH_SIZE), \"GT Hard\"\n",
    "else:\n",
    "    raise Exception(\"I don't know what kind of difficulty you're going for (you told me '{}')\".format(difficulty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "targets_loaded = []\n",
    "with torch.no_grad():\n",
    "    with SimType() as sim:\n",
    "        for batch_idx, batch_data in enumerate(test_loader):\n",
    "            zeta_batch = batch_data[0]\n",
    "            s_batch = batch_data[1]\n",
    "            print(\"max:\")\n",
    "            print(torch.max(zeta_batch[:, 1, 0]))\n",
    "            print(\"min:\")\n",
    "            print(torch.min(zeta_batch[:, 1, 0]))\n",
    "            print(\"mean:\")\n",
    "            print(torch.mean(torch.abs(zeta_batch[:, 1, 0] - zeta_batch[:, 0, 0])))\n",
    "            targets_loaded.append(zeta_batch[:, 1, 0].detach().cpu().float())\n",
    "            print(\"========\")\n",
    "print(targets_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Test Mean as Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.cat(targets_loaded, dim=0)\n",
    "print(\"overall target mean:\")\n",
    "mean = np.mean(targets.numpy())\n",
    "print(mean)\n",
    "constant_diffs = torch.abs(torch.tensor(mean).expand((len(targets))) - targets)\n",
    "print(np.mean(constant_diffs.cpu().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# TuneNet Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tunenet_gt\"\n",
    "outnames = [model_name + \"_history.pkl\", \"each_\" + model_name + \"_error.pkl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%notify -m \"finished testing TuneNet\"\n",
    "if recalc:\n",
    "    importlib.reload(tune.train_tunenet_gt)\n",
    "    \n",
    "    model = TuneNet(INPUT_DIM, OUT_DIM).to(get_torch_device())\n",
    "    model.load_state_dict(torch.load(os.path.join(output_path, model_name + \"_{}.pth\".format(TUNENET_EPOCHS))))\n",
    "    with SimType() as sim:\n",
    "        _, _, tunenet_history, each_tunenet_error = \\\n",
    "            tune.train_tunenet_gt.test(1, model, sim, test_loader,\n",
    "                                       tuning_iterations=tuning_iters,\n",
    "                                       display_graphs=False)\n",
    "\n",
    "    tunenet_history = tunenet_history.cpu().detach().numpy()\n",
    "\n",
    "    save_files(prefix, [tunenet_history, each_tunenet_error], outnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunenet_history, each_tunenet_error = load_files(prefix, outnames)\n",
    "fig, ax = plt.subplots()\n",
    "print(each_tunenet_error.shape)\n",
    "# ax.plot(np.mean(each_tunenet_error, axis=0, keepdims=False))\n",
    "ax.plot(np.swapaxes(each_tunenet_error[1:30, :5], 0, 1))\n",
    "print(np.amax(each_tunenet_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Direct Neural Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tunenet_gt_direct\"\n",
    "outnames = [model_name + \"_history.pkl\", \"each_\" + model_name + \"_error.pkl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify -m \"finished testing TuneNet Direct\"\n",
    "if recalc:\n",
    "    model = TuneNet(INPUT_DIM, OUT_DIM, degenerate=True).to(get_torch_device())\n",
    "    model.load_state_dict(torch.load(os.path.join(output_path, model_name + \"_{}.pth\".format(TUNENET_EPOCHS))))\n",
    "    with SimType() as sim:\n",
    "        _, _, tunenet_direct_history, each_tunenet_direct_error = \\\n",
    "            tune.train_tunenet_gt.test(1, model, sim, test_loader,\n",
    "                                       tuning_iterations=1,\n",
    "                                       display_graphs=False,\n",
    "                                       incremental=False)\n",
    "\n",
    "    tunenet_direct_history = tunenet_direct_history.cpu().detach().numpy()\n",
    "    save_files(prefix, [tunenet_direct_history, each_tunenet_direct_error],\n",
    "               outnames)\n",
    "print(tunenet_direct_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunenet_direct_history, each_tunenet_direct_error = load_files(prefix, outnames)\n",
    "# Tile the direct (single-iteration) tunenet result so its first dimension matches the other results\n",
    "tunenet_direct_history = np.pad(tunenet_direct_history,\n",
    "                                ((0, 0), (0, tuning_iters-tunenet_direct_history.shape[1]+1)),\n",
    "                                mode=\"edge\")\n",
    "each_tunenet_direct_error = np.pad(each_tunenet_direct_error,\n",
    "                                   ((0, 0), (0, tuning_iters-each_tunenet_direct_error.shape[1]+1)),\n",
    "                                   mode=\"edge\")\n",
    "fig, ax = plt.subplots()\n",
    "print(each_tunenet_direct_error.shape)\n",
    "# ax.plot(np.mean(each_tunenet_error, axis=0, keepdims=False))\n",
    "ax.plot(np.swapaxes(each_tunenet_direct_error[1:30, :5], 0, 1))\n",
    "print(np.amax(each_tunenet_direct_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMA-ES Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%notify -m \"finished CMA-ES\"\n",
    "if recalc:\n",
    "    from tune.cma import do_cma, do_cma_over_dataset\n",
    "    from tune.utils import exec_sim\n",
    "    import torch\n",
    "    importlib.reload(tune.cma)\n",
    "\n",
    "    with SimType() as sim:\n",
    "        cma_evals, cma_estimates, cma_targets = do_cma_over_dataset(test_loader, sim, maxfevals=SIM_EVALS, popsize=10)\n",
    "\n",
    "    save_files(prefix, [cma_evals, cma_estimates, cma_targets],\n",
    "               [\"cma_evals.pkl\", \"cma_estimates.pkl\", \"cma_targets.pkl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cma_evals, cma_estimates, cma_targets = load_files(prefix,\n",
    "           [\"cma_evals.pkl\", \"cma_estimates.pkl\", \"cma_targets.pkl\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Entropy Search Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify -m \"finished Greedy Entropy Search\"\n",
    "if recalc:\n",
    "    from tune.entsearch import entsearch_over_dataset\n",
    "    from tune.utils import get_timestamp\n",
    "    importlib.reload(tune.entsearch)\n",
    "\n",
    "    print(get_timestamp())\n",
    "    # range of possible parameter values\n",
    "    big_theta = np.linspace(np.min(targets.numpy()), np.max(targets.numpy()), 50)\n",
    "    epsilon = 0.001\n",
    "    print(big_theta)\n",
    "    # max number of sims\n",
    "    k_max = SIM_EVALS\n",
    "    # population size\n",
    "    n = 200\n",
    "\n",
    "    print(\"Performing entropy search...\")\n",
    "    entsearch_P_history, entsearch_estimates, entsearch_targets = None, None, None\n",
    "    with SimType() as sim:\n",
    "        entsearch_P_history, entsearch_estimates, entsearch_targets = \\\n",
    "            entsearch_over_dataset(test_loader, sim, big_theta, epsilon, k_max, n)\n",
    "    print(\"Entropy search complete.\")\n",
    "    print(get_timestamp())\n",
    "\n",
    "    save_files(prefix, [entsearch_P_history, entsearch_estimates, entsearch_targets],\n",
    "               [\"entsearch_P_history.pkl\", \"entsearch_estimates.pkl\", \"entsearch_targets.pkl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entsearch_P_history, entsearch_estimates, entsearch_targets = load_files(prefix,\n",
    "    [\"entsearch_P_history.pkl\", \"entsearch_estimates.pkl\", \"entsearch_targets.pkl\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Munge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cma_estimates)\n",
    "# print(type(cma_estimates))\n",
    "# print(type(cma_estimates[0,0]))\n",
    "\n",
    "# calculate the error for each tuning instance wrt that instance's target parameter value\n",
    "each_cma_error = np.abs(cma_targets[:, 0:1] - cma_estimates[:, :, 0])\n",
    "each_entsearch_error = np.abs(entsearch_targets[:, np.newaxis] - entsearch_estimates[:, :])\n",
    "# calculate mean error\n",
    "mean_cma_error = np.mean(each_cma_error, axis=0)\n",
    "mean_tunenet_error = np.mean(each_tunenet_error, axis=0, keepdims=False)\n",
    "mean_tunenet_direct_error = np.mean(each_tunenet_direct_error, axis=0, keepdims=False)\n",
    "mean_entsearch_error = np.mean(each_entsearch_error, axis=0, keepdims=False)\n",
    "\n",
    "# need two points to define a line for plotting, so duplicate the result\n",
    "each_constant_error = constant_diffs.unsqueeze(1).repeat([1, 2]).numpy()\n",
    "# also need x-values to plot\n",
    "constant_x = [0, SIM_EVALS]\n",
    "\n",
    "# find average across all runs\n",
    "mean_constant_error = np.mean(each_constant_error, axis=0, keepdims=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_plots = 20\n",
    "# cma_color = [1.0, 0.5, 0.0, 1.0]\n",
    "# tunenet_color = [0.0, 0.0, 1.0, 1.0]\n",
    "# tunenet_direct_color = [0.0, 0.7, 0.7, 1.0]\n",
    "# entsearch_color = [0.0, 0.5, 0.0, 1.0]\n",
    "\n",
    "\n",
    "tunenet_color = (np.array([230,97,1,255]) / 255).tolist()\n",
    "tunenet_direct_color = (np.array([253,184,99,255]) / 255).tolist()\n",
    "cma_color = (np.array([178,171,210,255]) / 255).tolist()\n",
    "entsearch_color = (np.array([94,60,153,255]) / 255).tolist()\n",
    "constant_color = [0.5, 0.5, 0.5, 0.5]\n",
    "\n",
    "label_list = [\"TuneNet\",\n",
    "              \"Direct Prediction\",\n",
    "              \"CMA-ES\",\n",
    "              \"EntSearch\",\n",
    "              \"Mean\",\n",
    "             ]\n",
    "error_x_list = [np.asarray(list(range(len(mean_tunenet_error)))),\n",
    "                np.asarray(list(range(len(mean_tunenet_direct_error)))),\n",
    "                cma_evals[0],\n",
    "                np.asarray(list(range(len(mean_entsearch_error)))),\n",
    "                constant_x,\n",
    "               ]\n",
    "error_y_list = [mean_tunenet_error,\n",
    "                mean_tunenet_direct_error,\n",
    "                mean_cma_error,\n",
    "                mean_entsearch_error,\n",
    "                mean_constant_error,\n",
    "               ]\n",
    "error_each_list = [each_tunenet_error,\n",
    "                   each_tunenet_direct_error,\n",
    "                   each_cma_error,\n",
    "                   each_entsearch_error,\n",
    "                   each_constant_error,\n",
    "                  ]\n",
    "color_list = [tunenet_color, \n",
    "              tunenet_direct_color,\n",
    "              cma_color,\n",
    "              entsearch_color,\n",
    "              constant_color,\n",
    "             ]\n",
    "        \n",
    "# some quality of life functions\n",
    "def lighten_value(val, amt):\n",
    "    return val + (1-val)*amt\n",
    "\n",
    "def faint(color, lighten=0.65, alpha=0.15):\n",
    "    return [\n",
    "        lighten_value(color[0], lighten),\n",
    "        lighten_value(color[1], lighten),\n",
    "        lighten_value(color[2], lighten),\n",
    "        alpha\n",
    "    ]\n",
    "\n",
    "# Traces figure\n",
    "for label, error_each, error_x, error_y, color in \\\n",
    "        zip(label_list, error_each_list, error_x_list, error_y_list, color_list):    \n",
    "    fig, ax = plt.subplots()\n",
    "    n_averaged = len(test_loader.dataset)\n",
    "    for e in error_each:\n",
    "        ax.plot(error_x, e, color=faint(color))\n",
    "    ax.plot(error_x, np.mean(error_each, axis=0), color=color, label=label)\n",
    "    \n",
    "    ax.tick_params(\n",
    "      axis='y',          # changes apply to the x-axis\n",
    "      which='both',      # both major and minor ticks are affected\n",
    "      right=False,      # ticks along the bottom edge are off\n",
    "      left=False,         # ticks along the top edge are off\n",
    "      labelbottom=False) # labels along the bottom edge are off\n",
    "    ax.get_xaxis().set_visible(True)\n",
    "    ax.get_xaxis().set_visible(True)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.set_xlabel(\"Simulation rollouts\")\n",
    "    ax.set_ylabel(\"COR value error\")\n",
    "    ax.set_xlim([0, 60*(error_x[1]-error_x[0])])\n",
    "    ax.set_ylim([0, 0.3])\n",
    "    ax.legend()\n",
    "    ax.set_title(label)\n",
    "    fig.tight_layout()\n",
    "    filename = 'iterations_vs_performance_{}_{}.pdf'.format(prefix,\n",
    "                                                            label.lower().replace('-', '').replace(' ', '_'))\n",
    "    fig.savefig(os.path.join(ROOT_DIR, OUTPUT_DIR, prefix, filename))\n",
    "\n",
    "    \n",
    "# report the best value over this many iterations\n",
    "min_over_iterations_list = [1, 5, 10, 100]\n",
    "print(\"N = \" + str(min_over_iterations_list))\n",
    "print(\"minimum value over the first N iterations\")\n",
    "for label, error_each, x, error_y, color in \\\n",
    "        reversed(list(zip(label_list, error_each_list, error_x_list, error_y_list, color_list))):\n",
    "\n",
    "    format_string = \"{}\"\n",
    "    mins = []\n",
    "    for n in min_over_iterations_list:\n",
    "        up_thru_idx = 1\n",
    "        for idx, xval in enumerate(x):\n",
    "            if xval <= n and idx > up_thru_idx:\n",
    "                up_thru_idx = idx\n",
    "        print(up_thru_idx)\n",
    "#         print(error_y)\n",
    "#         mins.append(np.min(error_y[:up_thru_idx]))\n",
    "        mins.append(error_y[up_thru_idx])\n",
    "        format_string += \" & {:.4f}\"\n",
    "    print(format_string.format((label + \" \"*20)[:20], *mins))\n",
    "\n",
    "    \n",
    "plt.rcParams.update({'font.size': 14})\n",
    "# Combined figure\n",
    "fig, ax = plt.subplots()\n",
    "for label, error_each, error_x, error_y, color in \\\n",
    "        reversed(list(zip(label_list, error_each_list, error_x_list, error_y_list, color_list))):\n",
    "    ax.plot(error_x, error_y, color=color, label=label, linewidth=2)\n",
    "\n",
    "# base line\n",
    "# ax.axhline(y=0, linestyle=\"--\", color=[0.0, 0.0, 0.0, 0.5], label=\"target\")\n",
    "ax.tick_params(\n",
    "  axis='y',          # changes apply to the x-axis\n",
    "  which='both',      # both major and minor ticks are affected\n",
    "  right=True,      # ticks along the bottom edge are off\n",
    "  left=True,         # ticks along the top edge are off\n",
    "  labelbottom=False) # labels along the bottom edge are off\n",
    "ax.get_xaxis().set_visible(True)\n",
    "ax.get_xaxis().set_visible(True)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.set_xlabel(\"Simulation rollouts\")\n",
    "ax.set_ylabel(\"Parameter MAE\".format(n_averaged))\n",
    "ax.set_xlim([-1, 100])\n",
    "# ax.set_ylim([0, 0.15])\n",
    "ax.legend()\n",
    "ax.set_title(graph_title)\n",
    "\n",
    "fig.tight_layout()\n",
    "filename = 'error_vs_iterations_{}_all.pdf'.format(prefix)\n",
    "fig.savefig(os.path.join(ROOT_DIR, OUTPUT_DIR, prefix, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midpoint_integration(x, y):\n",
    "    \"\"\"\n",
    "    Calculate area under a curve using midpoint summation.\n",
    "    :param x: vector of x-values\n",
    "    :param y: vector of y-values\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    for i in range(len(x)-1):\n",
    "        total += ((y[i+1] - y[i])/2 + y[i])*(x[i+1] - x[i])\n",
    "    return total\n",
    "\n",
    "# some quick tests\n",
    "print(midpoint_integration([1, 2, 3], [1, 1, 1])) # rectangle, should = 2\n",
    "print(midpoint_integration([1, 2, 3], [4, 0, 4])) # two triangles, should = 4\n",
    "# complex, should = 23.5\n",
    "print(midpoint_integration([0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 100], [3, 3, 1, 1, 4, 1, 2, 2, 4, 1, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from numpy.polynomial.polynomial import polyfit\n",
    "import scipy.stats\n",
    "\n",
    "print(\"Area under the error curve, which is averaged over 100 datapoints, 1 run per datapoint, N simulations/run\")\n",
    "\n",
    "error = {}\n",
    "padlen = max([len(l) for l in label_list]) + 4\n",
    "for n in [1, 2, 3, 5, 10, 20, 100]:\n",
    "    print(\"N = {} simulations:\".format(n))\n",
    "    _, ax_lin = plt.subplots()\n",
    "    _, ax_log = plt.subplots()\n",
    "    for typ, x, y, each_y, color in zip(label_list, error_x_list, error_y_list, error_each_list, color_list):\n",
    "        if n not in x:\n",
    "            print(\"  <skipping {}, cannot find data for exactly N={} simulations>\".format(typ, n))\n",
    "            continue\n",
    "        up_thru_idx = np.where(x==n)[0][0]+1\n",
    "        label_part = \"  {}:\".format(typ) + ' '*padlen\n",
    "        print(label_part[:padlen] + \n",
    "              \"{:.3f}\".format(midpoint_integration(x[:up_thru_idx], y[:up_thru_idx])))\n",
    "        if typ not in error:\n",
    "            error[typ] = {'err_initial': {}, 'err_final': {}}\n",
    "        error[typ]['err_initial'] = each_y[:, 0]\n",
    "        error[typ]['err_final'][n] = each_y[:, up_thru_idx-1]\n",
    "#         print(error[typ]['err_initial'].shape)\n",
    "        \n",
    "        ax_lin.set_title(\"N = {} simulations, y-axis linear\".format(n))\n",
    "        slope, intercept, r_value, p_value, std_err = \\\n",
    "            scipy.stats.linregress(error[typ]['err_initial'], error[typ]['err_final'][n])\n",
    "        endpoints = np.array([0, 0.7])\n",
    "        ax_lin.plot(endpoints, slope * endpoints + intercept, '-', color=faint(color, alpha=0.5, lighten=0.0))\n",
    "        fit_label = \", slope={:.2f}, $r^2$={:.2f}\".format(slope, r_value**2)\n",
    "        \n",
    "        ax_log.set_title(\"N = {} simulations, y-axis log\".format(n))\n",
    "        ax_log.set_yscale('log')\n",
    "        \n",
    "        for ax, label_suffix in zip([ax_lin, ax_log], [fit_label, \"\"]):\n",
    "            ax.scatter(error[typ]['err_initial'], error[typ]['err_final'][n], label=typ + label_suffix, color=color, s=3)\n",
    "            ax.legend()\n",
    "            ax.set_xlabel(\"initial guess absolute error\")\n",
    "            ax.set_ylabel(\"final prediction absolute error\")\n",
    "    print(\"\")\n",
    "            \n",
    "    # calculate mean and standard deviation of error across population\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%notify\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tunenet)",
   "language": "python",
   "name": "tunenet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
